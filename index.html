<!DOCTYPE HTML>
<html lang="en">
  <!-- Header -->
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Meng Song</title>
    <meta name="author" content="Meng Song">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>
  <!-- Body -->
  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr style="padding:0px">
          <td style="padding:0px">
            <!-- Intro -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr style="padding:0px">
                  <td style="padding:2.5%;width:63%;vertical-align:middle">
                    <p class="name" style="text-align: center;">
                      Meng Song
                    </p>
                    <p style="text-align: justify;">
                      I completed my PhD at <a href="http://www.cs.ucsd.edu/">UC San Diego</a>, where I was advised <a href="https://cseweb.ucsd.edu/~mkchandraker/">Prof. Manmohan Chandraker</a>.
                      <br>
                      <br>
                      Prior to that, I obtained my Master's degree at <a href="http://www.ri.cmu.edu/">the Robotics Institute, Carnegie Mellon University</a>, working with <a href="http://www.cs.cmu.edu/~abhinavg/">Prof. Abhinav Gupta</a> and <a href="https://www.linkedin.com/in/daniel-huber-22b2631/">Dr. Daniel Huber</a>.
                    </p>
                    <p style="text-align:center">
                      <a href="mailto:mes050@ucsd.edu">Email</a> &nbsp;/&nbsp;
                      <a href="data/meng_resume.pdf">CV</a> &nbsp;/&nbsp;
                      <a href="https://scholar.google.com/citations?user=UxuTHlcAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                      <a href="https://twitter.com/meng_song">Twitter</a> &nbsp;/&nbsp;
                      <a href="https://github.com/mengsong16">Github</a> &nbsp;/&nbsp;
                      <a href="https://www.linkedin.com/in/mengsong16">LinkedIn</a>
                    </p>
                  </td>
                  <td style="padding:2.5%;width:37%;max-width:37%">
                    <a href="images/photo_cropped.jpg"><img style="width:75%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/photo_cropped.jpg" class="hoverZoomLink"></a>
                  </td>
                </tr>
              </tbody>
            </table>
            <!-- Research interests -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p style="text-align: justify;">
                    My research is motivated by the goal of developing a mathematical construct of the intelligent agent from first principles. My recent work has primarily focused on answering the question "What is a good representation of states and goals in decision-making problems?" I explored this problem under three different learning paradigms: reinforcement learning, imitation learning, and unsupervised learning.
                  </p>
                </td>
              </tr>
              </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <!-- Publications -->
        <!-- RL generalize -->
        <tr>
          <td style="padding:16px;width:20%;vertical-align:middle">
            <div class="one">
              <img src='images/rl_generalize.png' width=100%>
            </div>
          </td>
          <td style="padding:8px;width:80%;vertical-align:middle">
            <span class="papertitle">Good Actions Succeed, Bad Actions Generalize: A Case Study on Why RL Generalizes Better</span>
            <br>
            <strong>Meng Song</strong>
            <br>
            <em>Preprint</em>, 2025
            <br>
            <a href="https://arxiv.org/abs/2503.15693">arXiv</a>
            /
            <a href="https://github.com/mengsong16/enlighten">code</a>
            <p></p>
            <p style="text-align: justify;">
            PPO and BC generalize differently in visual navigation: BC imitates successful trajectories, while PPO combinatorially stitches together past experiences, including failures, to solve new tasks and achieve stronger generalization.
            </p>
          </td>
        </tr> 
        <!-- Thesis -->
        <tr>
          <td style="padding:16px;width:20%;vertical-align:middle">
            <div class="one">
              <img src='images/phd_thesis.png' width=100%>
            </div>
          </td>
          <td style="padding:8px;width:80%;vertical-align:middle">
            <span class="papertitle">Towards Unsupervised Goal Discovery: Learning Plannable Representations with Probabilistic World Modeling</span>
            <br>
            <strong>Meng Song</strong>
            <br>
            <em>PhD Thesis</em>, 2024
            <br>
            <a href="https://escholarship.org/content/qt33g971rx/qt33g971rx_noSplash_f96408a792fe0a65a6af9d5d97370eb7.pdf">escholarship</a>
            <p></p>
            <p style="text-align: justify;">
              Learning through interaction is a foundational principle in both human and animal learning. 
              In a broad sense, intelligent agents can be formulated as goal-directed systems
              interacting with an uncertain environment. 
              <br>
              <br>
              Despite the generality of this definition, a key challenge in computationally grounding it lies in how to effectively set up and represent goals
              and purposes. This dissertation explores this question through the lens of various machine
              learning paradigms.
            </p>
          </td>
        </tr> 
        <!-- Subgoal -->
        <tr>
          <td style="padding:16px;width:20%;vertical-align:middle">
            <div class="one">
              <img src='images/subgoal.png' width=100%>
            </div>
          </td>
          <td style="padding:8px;width:80%;vertical-align:middle">
            <span class="papertitle">Probabilistic World Modeling with Asymmetric Distance Measure</span>
            <br>
            <strong>Meng Song</strong>
            <br>
            <em>Geometry-grounded Representation Learning and Generative Modeling Workshop at International Conference on Machine Learning (ICML)</em> &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
            <br>
            <a href="https://arxiv.org/abs/2403.10875">arXiv</a>
            <p></p>
            <p style="text-align: justify;">
              A novel probabilistic world model trained with contrastive learning. The learned latent space enables subgoal discovery, asymmetric transition modeling, and supports highly efficient planning without requiring any inference-time search.  
            </p>
          </td>
        </tr> 
        <!-- Minimalist Prompt -->
        <tr>
          <td style="padding:16px;width:20%;vertical-align:middle">
            <div class="one">
              <img src='images/minimal_prompt.png' width=100%>
            </div>
          </td>
          <td style="padding:8px;width:80%;vertical-align:middle">
            <span class="papertitle">A Minimalist Prompt for Zero-Shot Policy Learning</span>
            <br>
            <strong>Meng Song</strong>
            <a href="https://scholar.google.com/citations?user=ScLUQ-YAAAAJ&hl=en">Xuezhi Wang</a>,
            <a href="https://tanaybiradar.com/">Tanay Biradar</a>,
            <a href="https://yaoqin1.github.io/">Yao Qin</a>, 
            <a href="https://cseweb.ucsd.edu/~mkchandraker/">Manmohan Chandraker</a>
            <br>
            <em>Task Specification Workshop at The Robotics: Science and Systems (RSS)</em>, 2024
            <br>
            <a href="https://arxiv.org/abs/2405.06063">arXiv</a>
            /
            <a href="https://github.com/mengsong16/prompt_dt">code</a>
            <p></p>
            <p style="text-align: justify;">
            A novel prompting method that enables interpretable zero-shot generalization in unseen robotics tasks without requiring demonstrations and surpasses few-shot baselines.
            </p>
          </td>
        </tr>
        <!-- RL prompt -->
        <tr>
          <td style="padding:16px;width:20%;vertical-align:middle">
            <div class="one">
              <img src='images/prompt_rl.png' width=100%>
            </div>
          </td>
          <td style="padding:8px;width:80%;vertical-align:middle">
            <span class="papertitle">RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning</span>
            <br>
            <a href="https://mingkaid.github.io/">Mingkai Deng</a>*,
            <a href="https://scholar.google.com/citations?user=woyUwp0AAAAJ&hl=zh-CN">Jianyu Wang</a>*,
            <a href="https://hsiehjackson.github.io/">Cheng-Ping Hsieh</a>, 
            <a href="https://www.linkedin.com/in/yhwang0315/">Yihan Wang</a>
            <a href="https://han-guo.info/">Han Guo</a>
            <a href="https://www.tshu.io/">Tianmin Shu</a>
            <strong>Meng Song</strong>
            <a href="https://www.cs.cmu.edu/~epxing/">Eric P. Xing</a>
            <a href="https://scholar.google.com/citations?user=N7_xhHoAAAAJ&hl=en">Zhiting Hu</a>
            <br>
            <em>Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2022
            <br>
            <a href="https://arxiv.org/abs/2205.12548">arXiv</a>
            /
            <a href="https://blog.ml.cmu.edu/2023/02/24/rlprompt-optimizing-discrete-text-prompts-with-reinforcement-learning/">blog</a>
            <p></p>
            <p style="text-align: justify;">
            RL-based prompt optimization approach outperforms a wide range of finetuning and prompting baselines on text classification and style transfer tasks.
            </p>
          </td>
        </tr>
        <!-- Learn physics -->
        <tr>
          <td style="padding:16px;width:20%;vertical-align:middle">
            <div class="one">
              <img src='images/learn_physics.png' width=100%>
            </div>
          </td>
          <td style="padding:8px;width:80%;vertical-align:middle">
            <span class="papertitle">Learning to Rearrange with Physics-Inspired Risk Awareness</span>
            <br>
            <strong>Meng Song</strong>
            <a href="https://jaysparrow.github.io/">Yuhan Liu</a>,
            <a href="https://sites.google.com/view/zhengqinli">Zhengqin Li</a>,
            <a href="https://cseweb.ucsd.edu/~mkchandraker/">Manmohan Chandraker</a>
            <br>
            <em>Conference on Risk Aware Decision Making Workshop at The Robotics:  Science and Systems (RSS)</em>, 2022
            <br>
            <a href="https://arxiv.org/abs/2206.12784">arXiv</a>
            /
            <a href="https://github.com/mengsong16/openvrooms">code</a>
            /
            <a href="https://sites.google.com/view/physics-inspired-risk-aware">project page</a>
            <p></p>
            <p style="text-align: justify;">
            A PPO agent learns physical concepts such as mass and friction by actively interacting with the environment and mastering everyday skills, instead of passively observing physical processes.
            </p>
          </td>
        </tr>
        <!-- Openrooms -->
        <tr>
          <td style="padding:16px;width:20%;vertical-align:middle">
            <div class="one">
              <img src='images/openrooms.png' width=100%>
            </div>
          </td>
          <td style="padding:8px;width:80%;vertical-align:middle">
            <span class="papertitle">OpenRooms: An End-to-End Open Framework for Photorealistic Indoor Scene Datasets</span>
            <br>
            <a href="https://sites.google.com/view/zhengqinli">Zhengqin Li</a>,
            <a href="https://www.linkedin.com/in/ting-wei-yu/">Ting-Wei Yu</a>,
            <a href="https://ssangx.github.io/">Shen Sang</a>,
            <a href="https://www.linkedin.com/in/sawang07/">Sarah Wang</a>,
            <strong>Meng Song</strong>
            <a href="https://jaysparrow.github.io/">Yuhan Liu</a>,
            <a href="https://yuyingyeh.github.io/">Yu-Ying Yeh</a>, 
            <a href="https://jerrypiglet.github.io/">Rui Zhu</a>, 
            <a href="https://scholar.google.com/citations?user=v19p_0oAAAAJ&hl=en">Nitesh Gundavarapu</a>, 
            <a href="https://scholar.google.com/citations?user=asHobe0AAAAJ&hl=en">Jia Shi</a>, 
            <a href="https://sai-bi.github.io/">Sai Bi</a>, 
            <a href="https://zexiangxu.github.io/">Zexiang Xu</a>, 
            <a href="https://kovenyu.com/">Hong-Xing Yu</a>, 
            <a href="http://www.kalyans.org/">Kalyan Sunkavalli</a>, 
            <a href="http://www.miloshasan.net/">Miloš Hašan</a>, 
            <a href="https://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
            <a href="https://cseweb.ucsd.edu/~mkchandraker/">Manmohan Chandraker</a>
            <br>
            <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
            <br>
            <a href="https://arxiv.org/abs/2007.12868">arXiv</a>
            /
            <a href="https://github.com/ViLab-UCSD/OpenRooms?tab=readme-ov-file/">dataset</a>
            /
            <a href="https://vilab-ucsd.github.io/ucsd-openrooms/">project page</a>
            <p></p>
            <p style="text-align: justify;">
            A novel framework for creating large-scale photorealistic datasets of indoor scenes, enabling broad applications in inverse rendering, scene understanding, and robotics. 
            </p>
          </td>
        </tr>
        <!-- S4G -->
        <tr>
          <td style="padding:16px;width:20%;vertical-align:middle">
            <div class="one">
              <img src='images/s4g.png' width=100%>
            </div>
          </td>
          <td style="padding:8px;width:80%;vertical-align:middle">
            <span class="papertitle">S4G: Amodal Single-view Single-Shot SE(3) Grasp Detection in Cluttered Scenes</span>
            <br>
            <a href="https://yzqin.github.io/">Yuzhe Qin</a>,
            <a href="https://callmeray.github.io/homepage/">Rui Chen</a>,
            <a href="https://berniezhu.github.io/">Hao Zhu</a>,
            <strong>Meng Song</strong>
            Jing Xu,
            <a href="https://cseweb.ucsd.edu/~haosu/">Hao Su</a>
            <br>
            <em>Conference on Robot Learning (CoRL)</em>, 2019 &nbsp <font color="red"><strong>(Spotlight Presentation)</strong></font>
            <br>
            <a href="https://arxiv.org/abs/1910.14218">arXiv</a>
            <p></p>
            <p style="text-align: justify;">
            A novel 6-DoF grasp detection method using a single-shot grasp proposal network trained on automatically generated synthetic data, significantly outperforming state-of-the-art methods.
            </p>
          </td>
        </tr>
        <!-- thin -->
        <tr>
          <td style="padding:16px;width:20%;vertical-align:middle">
            <div class="one">
              <img src='images/thin.png' width=100%>
            </div>
          </td>
          <td style="padding:8px;width:80%;vertical-align:middle">
            <span class="papertitle">Automatic Recovery of Networks of Thin Structures</span>
            <br>
            <strong>Meng Song</strong>
            <a href="https://www.linkedin.com/in/daniel-huber-22b2631/">Daniel Huber</a>
            <br>
            <em>International Conference on 3D Vision (3DV)</em>, 2015 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
            <br>
            <a href="https://ieeexplore.ieee.org/document/7335467">paper</a>
            <p></p>
            <p style="text-align: justify;">
            A novel geometric approach that automatically segments and parses a complex network of thin structures from low-quality 3D point clouds.
            </p>
          </td>
        </tr>
        <!-- localization -->
        <tr>
          <td style="padding:16px;width:20%;vertical-align:middle">
            <div class="one">
              <img src='images/localization.png' width=100%>
            </div>
          </td>
          <td style="padding:8px;width:80%;vertical-align:middle">
            <span class="papertitle">Natural Feature based Localization in Forested Environments</span>
            <br>
            <strong>Meng Song</strong>
            <a href="https://csen.nankai.edu.cn/info/1028/1055.html">Fengchi Sun</a>
            <a href="https://www.linkedin.com/in/karl-iagnemma-b52186102/">Karl Iagnemma</a>
            <br>
            <em>International Conference on Intelligent Robots and Systems (IROS)</em>, 2012 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
            <br>
            <a href="https://ieeexplore.ieee.org/document/6385542">paper</a>
            <p></p>
            <p style="text-align: justify;">
            A novel geometric approach for extracting and modeling tree trunk landmarks from noisy 3D point clouds of cluttered forested environments.
            </p>
          </td>
        </tr>
        <!-- landmark -->
        <tr>
          <td style="padding:16px;width:20%;vertical-align:middle">
            <div class="one">
              <img src='images/landmark.png' width=100%>
            </div>
          </td>
          <td style="padding:8px;width:80%;vertical-align:middle">
            <span class="papertitle">Natural Landmark Extraction in Cluttered Forested Environments</span>
            <br>
            <strong>Meng Song</strong>
            <a href="https://csen.nankai.edu.cn/info/1028/1055.html">Fengchi Sun</a>
            <a href="https://www.linkedin.com/in/karl-iagnemma-b52186102/">Karl Iagnemma</a>
            <br>
            <em>International Conference on Robotics and Automation (ICRA)</em>, 2012 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
            <br>
            <a href="https://ieeexplore.ieee.org/document/6224680">paper</a>
            <p></p>
            <p style="text-align: justify;">
            A novel geometric approach for extracting and modeling tree trunk landmarks from noisy 3D point clouds of cluttered forested environments.
            </p>
          </td>
        </tr>
      </tbody>
    </table>

    <!-- Thoughts and Talks -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin:0 auto;">
      <tbody>
        <tr>
          <td>
            <h2>Thoughts and Talks</h2>
          </td>
        </tr>
        <tr>
          <td style="padding:16px 2px; width:100%;vertical-align:middle">
            Probabilistic World Modeling with Asymmetric Distance Measure, <a href="https://gram-workshop.github.io/schedule.html"><em></em>GRaM Workshop @ ICML</em></a>, 2024 
            &nbsp [<a href="data/slides_GRaM_ICML_2024.pdf">slides</a>] [<a href="data/poster_GRaM_ICML_2024.pdf">poster</a>]
            <br>
            Learning to Rearrange with Physics-Inspired Risk Awareness, <a href="https://sites.google.com/nyu.edu/risk-aware-decision-making.html"><em>RADM Workshop @ RSS</em></a>, 2022
            &nbsp [<a href="data/slides_RADM_RSS_2022.pdf">slides</a>]
            <br>
            Physics-Aware Robot Learning, <em>Thesis Proposal</em>, 2021
            &nbsp [<a href="data/thesis_proposal.pdf">slides</a>]
            <br>
            Finding Structure in Deep Reinforcement Learning, <em>Research Exam</em>, 2019
            &nbsp [<a href="data/research_exam.pdf">slides</a>]
            <br>
            One-shot Logo Detection in the Wild, <a href="https://www.wiml.org/wiml-workshop-2018"><em>WiML Workshop @ NeurIPS</em></a>, 2018
            &nbsp [<a href="data/WIML_2018.pdf">poster</a>]
            <br>
            Bridging Computational Complexity and Machine Learning, <a href="https://cseweb.ucsd.edu/classes/wi18/cse200-a/"><em>CSE 200: Computability and Complexity</em></a>, 2018
            &nbsp [<a href="data/complexity.pdf">paper</a>]
            <br>
            The Syntactic Mechanism Behind Image Captioning, <em>Research Project</em> with <a href="https://xinleic.xyz/">Xinlei Chen</a>, 2015
            &nbsp [<a href="data/captioning.pdf">paper</a>]
          </td>
        </tr>
      </tbody>
    </table>

    <!-- Teaching -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin:0 auto; margin-right:auto;margin-left:auto;">
      <tbody>
        <tr>
          <td>
            <h2>Teaching</h2>
          </td>
        </tr>
        <tr>
          <td style="padding:16px 2px; width:100%; vertical-align:middle">
            <a href="https://cseweb.ucsd.edu/~dasgupta/251u/">CSE 251-U: Unsupervised Learning</a>&nbsp Teaching Assistant, Winter 2024
            <br>
            <a href="http://zhiting.ucsd.edu/teaching/dsc250fall2023/">DSC 250: Advanced Data Mining</a>&nbsp Teaching Assistant, Fall 2023
            <br>
            <a href="https://cseweb.ucsd.edu/~dasgupta/251u/index.html">CSE 291-F: Unsupervised Learning</a>&nbsp Teaching Assistant, Spring 2023
            <br>
            <a href="https://cseweb.ucsd.edu/classes/wi23/cse203B-a/">CSE 203-B: Convex Optimization</a>&nbsp Teaching Assistant, Winter 2023
            <br>
            <a href="https://cseweb.ucsd.edu/~mkchandraker/classes/CSE152A/Fall2022/">CSE 152-A: Introduction to Computer Vision</a>&nbsp Teaching Assistant, Fall 2022
            <br>
            <a href="https://cseweb.ucsd.edu/~mkchandraker/classes/CSE252D/Spring2022/">CSE 252-D: Advanced Computer Vision</a>&nbsp Teaching Assistant, Spring 2022
            <br>
            <a href="https://cseweb.ucsd.edu//classes/wi22/cse203B-a/">CSE 203-B: Convex Optimization</a>&nbsp Teaching Assistant, Winter 2022
            <br>
            <a href="http://zhiting.ucsd.edu/teaching/dsc190fall2021/">DSC 190: Machine Learning with Few Labels</a><strong>(first offering)</strong>&nbsp Teaching Assistant, Fall 2021
            <br>
            <a href="https://cseweb.ucsd.edu/classes/sp21/cse8A-c/?_ga=2.159978045.1277837079.1748981667-836071369.1585711564&_gl=1*95bneo*_ga*ODM2MDcxMzY5LjE1ODU3MTE1NjQ.*_ga_DQLWSKCKE6*czE3NDkwNjI1MjIkbzI2JGcxJHQxNzQ5MDYyNjI3JGo2MCRsMCRoMA..*_gcl_au*MTU3OTkzNTQwNC4xNzQ4OTgxNjY3*_ga_PWJGRGMV0T*czE3NDkwNjI1MjIkbzMzJGcxJHQxNzQ5MDYyNjI3JGo2MCRsMCRoMA..">CSE 8A: Introduction to Programming 1</a>&nbsp Teaching Assistant, Spring 2021
            <br>
            <a href="https://cseweb.ucsd.edu/~dasgupta/251u/index.html">CSE 291-C: Probabilistic Approaches to Unsupervised Learning</a>&nbsp Teaching Assistant, Fall 2020
            <br>
            <a href="https://geoml.github.io/">CSE 291-F: Machine Learning Meets Geometry Data</a>&nbsp Teaching Assistant, Winter 2020
            <br>
            <a href="https://cse291-i.github.io/">CSE 291-I: Machine Learning on 3D Data</a><strong>(first offering)</strong>&nbsp Teaching Assistant, Winter 2019
            <br>
            <a href="https://ucsd-cse-152.github.io/FA20/index.html">CSE 152-A: Introduction to Computer Vision</a><strong>(first offering)</strong>&nbsp Teaching Assistant, Fall 2018
            <br>
          </td>
        </tr>
      </tbody>
    </table>

    <!-- Acknowledgement -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr>
          <td style="padding:0px">
            <br>
            <p style="text-align:right;font-size:small;">
              Template stolen from <a href="https://github.com/jonbarron/jonbarron.github.io">Jon Barron</a>.
            </p>
          </td>
        </tr>
      </tbody>
      </table>
      </td>
      </tr>
    </table>

  </body>
</html>
